= Marco Teórico

*POR HACER: DESCHAMULLAR Y AÑADIR REFERENCIAS DIRECTAS A MODELOS UTILIZADOS*
#v(1cm)
== Modelos de Lenguaje Multimodales
#v(.5cm)
// Los modelos de lenguaje multimodales han emergido como herramientas clave para el análisis semántico de contenidos visuales. Arquitecturas como CLIP (Contrastive Language-Image Pre-Training), BLIP (Bootstrapping Language-Image Pre-training) y CLAP (Contrastive Language-Audio Pre-Training) representan avances significativos al generar representaciones conjuntas de imágenes, texto y audio mediante espacios latentes compartidos. Estos modelos, entrenados en extensos corpus multimodales, no solo identifican objetos y atributos individuales, sino que también capturan relaciones espaciales, estilos artísticos y elementos emocionales en las composiciones. Su capacidad para producir descripciones contextualizadas y estructuradas ha sido documentada en trabajos recientes (Radford et al., 2021; Li et al., 2022; Wu et al., 2023), demostrando su potencial para aplicaciones en accesibilidad visual. La literatura especializada destaca cómo estas tecnologías superan limitaciones de modelos anteriores al integrar razonamiento visual, alineación semántica entre modalidades y la capacidad de vincular representaciones auditivas, sentando las bases para sistemas de descripción artística automatizada más completos e inmersivos.

//Los modelos de lenguaje multimodales constituyen la base de los sistemas capaces de procesar simultáneamente información visual y textual. Su funcionamiento se sustenta en arquitecturas basadas en transformers, que permiten integrar representaciones vectoriales provenientes de imágenes con aquellas generadas desde el lenguaje natural. Esta combinación posibilita tareas como la descripción automática de escenas, el análisis semántico de elementos visuales y la interpretación contextual de obras de arte. En el marco de este proyecto, estos modelos cumplen un rol clave al identificar componentes narrativos relevantes —figuras, objetos, colores, estilos, composiciones— y traducirlos en textos comprensibles y estructurados para su posterior conversión a sonido. La capacidad de razonamiento visual de estos modelos, junto con su alineación semántica entre modalidades, posibilita generar descripciones coherentes, accesibles y culturalmente informativas, fundamentales para la apreciación artística auditiva destinada a personas con discapacidad visual.
#v(.5cm)
== Modelos Generativos de Audio
#v(.5cm)
// Los modelos generativos de audio permiten crear paisajes sonoros, texturas acústicas y ambientes inmersivos a partir de instrucciones textuales o representaciones visuales. Su funcionamiento se basa en técnicas como modelos difusivos, autoencoders vectoriales (VQ-VAE) y transformadores autorregresivos, los cuales aprenden a sintetizar ondas sonoras o espectrogramas con características semánticas controladas. Estas tecnologías evolucionan continuamente, incorporando espacios latentes compartidos entre texto, imagen y audio, lo que facilita generar sonidos alineados con el contenido visual de una obra. Dentro del desarrollo de la plataforma, estos modelos permiten recrear atmósferas coherentes con el estilo, época o escena representada, ampliando la experiencia auditiva más allá de la narración descriptiva. El uso de estos sistemas posibilita construir una dimensión sonora rica, inmersiva y evocadora, complementando la comprensión estética de personas que dependen principalmente de la audición para acceder al arte visual.
#v(.5cm)
== Estándares de Accesibilidad
#v(.5cm)
// Los estándares de accesibilidad proporcionan el marco normativo que guía el diseño de interfaces inclusivas, asegurando que los contenidos digitales sean utilizables por personas con diversas capacidades sensoriales, motoras o cognitivas. Entre los más relevantes se encuentran las Pautas de Accesibilidad para el Contenido Web (WCAG), que establecen criterios relacionados con la perceptibilidad, operabilidad, comprensibilidad y robustez de los sistemas web. En el contexto chileno, estas directrices se integran mediante la Ley N.º 20.422, que regula la igualdad de oportunidades e incorpora exigencias específicas para tecnologías asistivas. La plataforma desarrollada en este proyecto adopta estos lineamientos para garantizar compatibilidad con lectores de pantalla, navegación por teclado, descripciones alternativas, controles auditivos accesibles y una estructura de interacción que facilite la exploración autónoma. El cumplimiento de estos estándares es esencial para asegurar que la solución propuesta no solo sea técnicamente avanzada, sino también verdaderamente inclusiva y centrada en las necesidades de la comunidad usuaria.

#pagebreak()
